\chapter{Baseline setup}

\section{Overview}
There are three software baselines provided by the DIHARD organizers, each for the parts of speech enhancement, speech activity detection and diarization. The speech enhancement baseline and the speech activity detection together are meant to be used in the case of system SAD (tracks 2 and 4), but since we only work with reference SAD, we do not need them. Thus we will only describe the diarization baseline in the following sections.

The diarization baseline is based on the best performing submission from John Hopkins University (JHU) in the previous year's DIHARD challenge (DIHARD I). There are 4 Kaldi recipes, each for an evaluation track, but we will focus only on the recipe for Track 1 since we only work with single channel and gold speech segmentation.

The baseline code modifies and reuses the \texttt{egs/dihard\_2018} recipe that was checked into Kaldi by the researchers at JHU. It does this by copying over new scripts and data that is needed to the \texttt{egs/dihard\_2018} directory and running the recipe from there.

This baseline code is in turn modified to make it easier to run with different trained models and parameters. All the diarization runs are run in this way.

\section{Initial segmentation}
The initial segmentation step deals with identifying speech and non-speech segments from the recording files using the reference SAD which is provided in the form of HTK label files. This results in a bunch of segments which are known to be containing only speech. These are called ``utterances" in Kaldi terminology and act as keys in the \texttt{utt2spk}, \texttt{feats.scp} and \texttt{segments} files. These reside in a Kaldi ``data directory", one for each dev and eval.

\section{Features}
The baseline then extracts 30 dimensional MFCC features for each of the every 10 ms using a 25 ms window. It uses the standard \texttt{steps/make\_mfcc.sh} Kaldi script for this. Later, cepstral mean and variance normalization (CMVN) with a 3 second sliding window is applied using the \texttt{apply-cmvn-sliding} Kaldi tool.

\section{Main segmentation}
After feature extraction, the utterances are uniformly divided into smaller 1.5 second subsegments with a 0.75 second overlap. This creates another Kaldi data directory with newer keys corresponding to each subsegment. An x-vector is extracted from each of these subsegments in the next step.

\section{Speaker representation for utterances}
The baseline extracts an x-vector from each subsegment using an x-vector extractor that is trained on the datasets Voxceleb I and II, along with added augmentation. The augmentation is done by additive noise (music, babble) from the MUSAN dataset and reverberation from the RIR dataset. The extraction script is \texttt{egs/callhome\_diarization/v1/diarization/nnet3/xvector/extract\_xvectors.sh}.

The baseline uses x-vectors \cite{snyder2018x} to represent speakers for each utterance. The x-vector extractor is a neural network that has a 512-dimensional embedding layer from which the x-vectors are extracted. This neural network is trained using data. The neural network is trained to discriminitively predict the correct speaker, given a chunk of frames at a time from the training data.

\section{Domain adaptation}
To adapt the extracted x-vectors to the DIHARD domain, they are normalized with a global mean and whitening transform that is learned from the DIHARD development set.

\section{Scoring}
The x-vectors are scored using a PLDA backend that is trained on a subset of Voxceleb consisting of segments of at least 3 seconds duration. This is done using the existing script \texttt{egs/callhome\_diarization/v1/diarization/score\_plda.sh}. These scores are stored as affinity matrices which give the scores between any pair of x-vectors.

\section{Clustering}
The x-vectors are then clustered using agglomerative hierarchical clustering (AHC) and a parameter sweep is done on the dev set to find the threshold that maximises the DER on the dev set. This threshold is then used for clustering the x-vectors of the eval set. The script used for clustering is \texttt{egs/callhome\_diarization/v1/diarization/cluster.sh}.

\section{Diarization output}
The clustering output is used to generate RTTMs using \texttt{egs/callhome\_diarization/v1/diarization/make\_rttm.py}. The RTTMs give a flat segmentation of the recordings with no overlap. Since the x-vectors were extracted from segments that were overlapping, care needs to be taken when two adjacent segments are assigned to a different speaker. The script places the speaker boundary midway between the end of the first segment and the start of the second segment.



