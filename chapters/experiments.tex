\chapter{Experiments and Results}
	\section{Modifications to the baseline scripts}
	
	\section{Baseline results}
	The baseline results shown in Table \ref{table-baseline-scores} have been computed by running the 2019 baseline on the datasets from the previous year's DIHARD challenge (2018). This is because it was not possible to register for DIHARD 2019 before the deadline, and thus access to 2019 datasets was denied.
	
	Luckily the 2018 datasets were released as a part of the June LDC newsletter. Since the basic problem statement of the challenge remains the same as last year, the last year's datasets can still be used without any trouble. But this unfortunately means that it is no longer possible to verify the computed baseline scores with official scores, because the official scores only exist for the 2019 datasets. The only useful hint was found on the webpage at \cite{dihard2-unofficial}, which mentions a rough score of 20.71 on the 2018 development set.
	
	The baseline (and all its modifications that use x-vectors) takes about 15 minutes to run on a 32-core machine with 60GB of memory.
	
	\begin{table}[h]
		\centering
		\begin{tabular}{|c|c|c|c|}
			\hline
			Dataset & DER (\%) & Speaker error (\%) & JER (\%) \\
			\hline
			dev & 19.96 & 11.62 & 53.92 \\
			\hline
			eval & 26.58 & 17.05 & 59.44 \\
			\hline
		\end{tabular}
	\caption{Baseline scores.}
	\label{table-baseline-scores}
	\end{table}
	
	The results already seem pretty good, considering that the system is not very complex. The best JHU system in DIHARD 2018 had an eval DER of 23.73\%, and that included doing Variational Bayes refinement as an extra step.

	It is also important to mention that Missed Speech and False Alarm are not mentioned in any of the following experiments, because they are always constant (since we work with reference SAD only). Missed speech is always 8.34\% for the dev set and 9.52\% for the eval set. False alarm is always zero for both, as expected. Missed speech should also be zero from the same reason, but it is not because the reference segmentation, which is in the form of HTK label files, have been created by merging overlapping segments. Since overlap detection is not done in any experiement and only flat segmentations (no overlap) are generated, this causes missed speech errors wherever these is overlap. This causes also means that 8.34\% and 9.52\% of the total speech duration (dev and eval respectively) is overlapping speech.
		
	\section{Using Existing Pre-trained Models}
	The first set of experiments was to see how certain pre-trained models freely available on the Internet perform.
	
		\subsection{Kaldi VoxCeleb x-vector model}
			This model was downloaded from the Kaldi models webpage \cite{kaldi_models_webpage}, and closely follows the recipe in \cite{snyder2018x}. The recipe used for training is available in Kaldi at \ttvar{egs/voxceleb/v2}. Similar to the baseline recipe, this model is also trained on a combination of VoxCeleb I and II along with augmentation. Both produce 512-dimensional x-vectors. There are two differences though:
			\begin{itemize}
				\item The PLDA backend included in this model is trained on the whole training data, which consists of 1,276,888 utterances. The baseline PLDA backend is trained on a subset where each segment is at least 3 seconds long.
				\item The PLDA backend here is trained on 200 dimensional x-vectors, which are produced after LDA. The baseline PLDA backend is trained directly on 512-dimensional x-vectors.
			\end{itemize}
			
			The results of the diarization recipe modified to use this x-vector extractor DNN are given in Table \ref{table-kaldi-xvec}.
	
			\begin{table}[h]
			\centering
			\begin{tabular}{|c|c|c|c|}
				\hline
				Dataset & DER (\%) & Speaker error (\%) & JER (\%) \\
				\hline
				dev & 22.86 & 14.52 & 51.74 \\
				\hline
				eval & 26.42 & 16.89 & 55.55 \\
				\hline
			\end{tabular}
			\caption{Scores with Kaldi VoxCeleb x-vector model.}
			\label{table-kaldi-xvec}
			\end{table}
		
			This has already produced a small improvement over the baseline. This could be possibly due to larger amount of training data used to train the PLDA backend, but more likely is due to indeterminism.
		
		\subsection{Kaldi VoxCeleb i-vector model}
			This model was also downloaded from the same Kaldi models webpage as the previous recipe, and closely follows the recipe in \cite{snyder2018x}. The recipe used for training is available in Kaldi at \ttvar{egs/voxceleb/v1}. Similar to the x-vector recipe, the model is also trained on a combination of VoxCeleb I and II, but without augmentation. This is because i-vectors are unable to effectively utilize data augmentation, unlike x-vectors \cite{snyder2018x}. The UBM is trained on 2048 gaussians using all the training utterances. The i-vector extractor is trained using the longest 100k utterances and produces 400-dimensional i-vectors. I-vectors are extracted for all the training utterances, reduced to 200 dimensions using LDA, and then used to train a PLDA backend.
			
			The results of the diarization recipe modified to use this i-vector extractor are given in Table \ref{table-kaldi-ivec}. Extracting i-vectors for the dev and eval set take much longer than extracting x-vectors, so this took around 6 hours. Every diarization run with i-vectors took a similar amount of time.
			
			\begin{table}[h]
				\centering
				\begin{tabular}{|c|c|c|c|}
					\hline
					Dataset & DER (\%) & Speaker error (\%) & JER (\%) \\
					\hline
					dev & 26.18 & 17.83 & 59.81 \\
					\hline
					eval & 32.03 & 22.51 & 65.22 \\
					\hline
				\end{tabular}
				\caption{Scores with Kaldi VoxCeleb i-vector model.}
				\label{table-kaldi-ivec}
			\end{table}
		
		Clearly, this is much worse than the x-vector model trained on the same VoxCeleb data without augmentation. Augmentation is not used because i-vectors are not able to effectively use data augmentation, unlike x-vectors \cite{snyder2018x}.
	
	\section{Training Custom Models}
	Training our own models was considered because that would allow models to be trained on in-domain data (the DIHARD development set). This should possibly give better results, considering that the model would get a chance to train on several different domains, rather than training on only one domain. The amount of data available in the dev set is relatively small, only 19 hours, so we do not expect great results, but it is worth a shot. The recipes in \ttvar{egs/voxceleb} were used as a starting point for both i-vector extractor and x-vector extractor training.
	
		\subsection{Training with DIHARD development set}
			The following results in Table \ref{table-dev-xvec} were obtained by training an x-vector model on the DIHARD development set. The reference RTTM files for the dev set were used to generate 28241 training utterances from 221 speakers. The recipe imposes a minimum feature length of 400 frames and a minimum 8 utterances per speaker, so after filtering only 2726 utterances from 90 speakers were used to train with. This meant that the trained DNN had 90 output nodes. The embedding layer had 512 dimension. X-vectors were extracted from the full dev set and reduced to 200 dimensions using LDA before training a PLDA backend.
			\begin{table}[h]
				\centering
				\begin{tabular}{|c|c|c|c|}
					\hline
					Dataset & DER (\%) & Speaker error (\%) & JER (\%) \\
					\hline
					dev & 41.35 & 33.00 & 74.60 \\
					\hline
					eval & 43.16 & 33.64 & 75.96 \\
					\hline
				\end{tabular}
				\caption{Scores with x-vector model trained on DIHARD dev.}
				\label{table-dev-xvec}
			\end{table}
		
		These seem to be pretty bad, but they align with the known fact that x-vectors perform poorly on small amounts of data, as mentioned in the conclusion of \cite{huang2018jhu}. As an additional experiment, augmentation was applied to the dev set. The augmentation was done similar to what the baseline does: 4 variants of the training set were created (reverb, noise, babble, music) and added to the original set, multiplying the number of utterances by 5. This resulted in 141205 utterances from 221 speakers, reduced to 13630 utterances from 90 speakers after filtering. This increased amount of training data resulted in a small increase in performance, as given in Table \ref{table-dev-xvec-aug}. But it still did not increase the perfomance beyond the baseline.
		
		\begin{table}[h]
			\centering
			\begin{tabular}{|c|c|c|c|}
				\hline
				Dataset & DER (\%) & Speaker error (\%) & JER (\%) \\
				\hline
				dev & 35.54 & 27.20 & 76.56 \\
				\hline
				eval & 39.43 & 29.91 & 78.65 \\
				\hline
			\end{tabular}
			\caption{Scores with x-vector model trained on DIHARD dev + augmentation.}
			\label{table-dev-xvec-aug}
		\end{table}
	
		Next, an i-vector model was trained on the dev set. Knowing that i-vectors perform better on relatively small amounts of data compared to x-vectors \cite{huang2018jhu}, it was expected to have better performance than the previous experiment. And it perform much better, as given in the Table \ref{table-dev-ivec}.
		
		\begin{table}[h]
			\centering
			\begin{tabular}{|c|c|c|c|}
				\hline
				Dataset & DER (\%) & Speaker error (\%) & JER (\%) \\
				\hline
				dev & 25.22 & xx.xx & 58.58 \\
				\hline
				eval & 34.61 & xx.xx & 65.69 \\
				\hline
			\end{tabular}
			\caption{Scores with i-vector model trained on DIHARD dev.}
			\label{table-dev-ivec}
		\end{table}
		
		This confirmed the findings from \cite{huang2018jhu} that the i-vector model without augmentation performs better than the x-vector model with augmentation, if the amount of available data is small.
		
		Training the in-domain i-vector model with augmentation was also considered during the last stages of the dissertation. The i-vector training on the the non-augmented dev set took 17 hours on a 32-core machine, with near 100\% CPU usage all the time. Thus due to lack of time, i-vector training was not attempted with an augmented dev set, which would be 5 times bigger.
		
		\subsection{Training with combination of VoxCeleb and DIHARD development set}
			For the next set of experiments the amount of training data was increased by adding data from VoxCeleb I. VoxCeleb II was not used because it is 7 times bigger than VoxCeleb I, making the training set too big, especially for i-vector training. There are 153,516 utterances from 1,251 speakers in VoxCeleb I, so this increases the total amount of training data significantly. All the parameters of the training remained unchanged.
			
			The results of the x-vector model trained on the combination of VoxCeleb I and DIHARD dev set are given in Table \ref{table-voxdev-xvec}.
			
			\begin{table}[h]
				\centering
				\begin{tabular}{|c|c|c|c|}
					\hline
					Dataset & DER (\%) & Speaker error (\%) & JER (\%) \\
					\hline
					dev & 23.45 & 15.11 & 56.89 \\
					\hline
					eval & 29.44 & 19.92 & 61.37 \\
					\hline
				\end{tabular}
				\caption{Scores with x-vector model trained on combination of VoxCeleb I and DIHARD dev.}
				\label{table-voxdev-xvec}
			\end{table}
		
		Now the amount of labelled training data available for x-vector training is not small, and it showed a big improvement in performance compared to the model that was just trained on the DIHARD dev set. But it still does not cross the baseline, which was trained with a much larger amount of data.
		
		It was also attempted to train an x-vector model after adding augmentations to the combination of VoxCeleb I and DIHARD dev. But attempts for that were unsuccessful as the neural network training kept getting interrupted because of memory allocation failures on the GPU. This was most likely because the GPUs on the grid were shared with other uses, causing available GPU memory to be low. This likely did not happen when training with just the dev set because the training was much faster, reducing the chances that someone submits a parallel GPU job. This could possibly be fixed by setting the GPU to exclusive mode using $nvidia-smi -c 3$, but superuser access was needed for that.
				
		An i-vector model was also trained on the combination of VoxCeleb I and DIHARD dev set. The results are given in Table \ref{table-voxdev-ivec}. There was an improvement compared to the model only trained on the dev set, which is expected.
		
		\begin{table}[h]
			\centering
			\begin{tabular}{|c|c|c|c|}
				\hline
				Dataset & DER (\%) & Speaker error (\%) & JER (\%) \\
				\hline
				dev & 25.15 & 16.81 & 56.78 \\
				\hline
				eval & 31.61 & 22.08 & 60.74 \\
				\hline
			\end{tabular}
			\caption{Scores with i-vector model trained on combination of VoxCeleb I and DIHARD dev.}
			\label{table-voxdev-ivec}
		\end{table}
		
	\section{Feature concatenation}
	For the final set of experiments, a new approach was tried for speaker embeddings. No new i-vector or x-vector extractor was trained in these experiments. Instead, the Kaldi pre-trained models were used to extract both i-vectors and x-vectors from the combination of VoxCeleb I and DIHARD dev set. These two sets of vectors were concatenated to form a set of new vectors (called c-vectors henceforth for simplicity). These c-vectors were reduced in dimension using LDA and later used to train a PLDA backend using the speaker labels available. To accommodate for this change, a new possible value was added to the $--vector_type$ argument in the diarization challenge recipe, which resulted in the extraction of both i-vectors and x-vectors from the dev and eval sets and concatenation.
	
	This new PLDA model was trained with varying dimensions from 200 up to 800. The results are given below in Table \ref{table-cvector}.
	\begin{table}[h]
		\centering
		\begin{tabular}{|c|c|c|c|c|c|c|}
			\hline
			Dim & DER(dev) & DER(eval) & S.Err(dev) & S.Err(eval) & JER(dev) & JER(eval) \\
			\hline
			200 & 26.40 & 28.18 & 18.05 & 18.65 & 51.77 & 54.66 \\
			300 & 25.11 & 26.64 & 16.76 & 17.12 & 49.02 & 52.48 \\
			400 & 25.25 & 26.63 & 16.91 & 17.10 & 47.65 & 52.54 \\
			500 & 26.23 & 25.52 & 17.89 & 15.99 & 47.58 & 50.30 \\
			600 & 25.53 & 25.22 & 17.19 & 15.70 & 47.17 & 50.27 \\
			700 & 24.88 & 24.95 & 16.54 & 15.42 & 47.11 & 50.85 \\
			800 & 23.62 & 24.64 & 15.27 & 15.12 & 46.63 & 51.05 \\
			\hline
		\end{tabular}
		\caption{Scores with c-vector PLDA backend trained on c-vectors extracted from combination of VoxCeleb I and DIHARD dev.}
		\label{table-cvector}
	\end{table}

	This best model is when c-vectors are 800 dimensional, at 24.64\% DER on the eval set. This is a significant improvement over the baseline, which is almost 2\% higher.
	
	\section{Hyperparameters}
		This sections describes the hyperparameters that were used to run the above experiments. Due to lack of time, it was not possible to experiment with tuning any of these and the defaults existing in the baseline or the reference training recipes were used.\\
		\subsection{MFCC feature vector length}
			The x-vector systems used 30 dimensional MFCCs because Kaldi's pretrained x-vector extractor DNN had 30 inputs. For consistency, the MFCC dimensionality for the the self trained x-vector extractor DNN was also set to 30.
			Similarly, the i-vector systems used 24 dimensional MFCCs because Kaldi's pre-trained UBM and i-vector extractor was trained with 24 dimensional Gaussians. The self trained i-vector extractor also used the same.
		\subsection{Speaker Embedding Length}
			This is an obvious one because it has a significant effect on the amount of speaker information that is present in each embedding. The Kaldi pre-trained models extracted 400 dimensional i-vectors, and the self-trained i-vector extractor was configured to do the same. Similarly in the x-vector case it was 512.
		\subsection{Segment Length and Overlap}
		The baseline diarization system used uniform segmentation with segment length 1.5 sec and 0.75 sec overlap (50\%). This was not changed in any of the diarization runs.
		\subsection{Clustering Threshold}
		The threshold during the clustering step in diarization is in terms of the log likelihood ratio provided by PLDA scores. In a perfectly caliberated system, the score is 0. The best threshold was selected from the list -0.5, -0.4, -0.3, -0.2, -0.1, -0.05, 0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5 using the computed DER on the dev set.
		\subsection{Number of UBM Gaussians}
		The number of Gaussians in the self-trained trained UBM was always 2048.
		\subsection{PLDA dimension}
		The dimensionality of the PLDA backend was always 200, except in the c-vectors experiment where it varied from 200 to 800.

	\section{Discussion of results}
		Three main experiements were carried out. The first included running the baseline with pre-trained i-vector and x-vector models available on the Kaldi website that have been trained on a large amount of data from the VoxCeleb dataset. The VoxCeleb dataset consists of audio taken from YouTube videos recorded in a variety of conditions, so there is significant channel, environment, and speaker variance. Other baseline parameters like segment length, overlap, clustering threshold, PLDA dimension were left unchanged. The x-vector model resulted in a similar performance as the baseline (26.42\%), and the i-vector model performed much worse with 32.03\%. This is expected as x-vectors have been found to out-perform i-vectors \cite{snyder2018x}, especially with data augmentation.
		
		The second experiment involved training own models for i-vector and x-vector extraction. The motivation for this was to take advantage of the in-domain data (the DIHARD dev set) to have better suited models for the DIHARD eval set, the composition of which is similar to the dev set. There were two parts to this - in the first part, these models were trained on only the DIHARD dev set. This was predicted to not have performance as the dev set has only 19 hours of audio from about 200 speakers, which is really small especially for the x-vector model. As predicted, the x-vector model's performance was terrible at 43.16\% (without augmentation) and 39.43\% (with augmentation). The i-vector model's performance was much better at 34.61\% suggesting that i-vectors perform better with small amounts of data. In the second part, the models were trained on a combination of VoxCeleb I and the DIHARD dev set. The goal was to increase the amount of training data using an external source, yet retaining some in-domain data. Both the x-vector and i-vector models were trained using this combination (without augmentation) and the results showed a good improvement with 29.44\% and 31.61\% respectively.
		
		The third experiment involved reusing Kaldi's i-vector and x-vector extractors (since they performed the best) but training own PLDA backend by using concatenated i-vectors and x-vectors. These new vectors were called c-vectors for simplicity. The i-vectors and x-vectors were extracted from the combination of VoxCeleb I and the DIHARD dev set. The baseline was also modified to extract both i-vectors and x-vectors from the dev and eval sets, concatenate them to get c-vectors, and score them using the new PLDA backend. Various dimensions of c-vectors were tried and the best performing was 800 at 24.64\%. This is 2\% lower than the baseline score which is at 26.58\%.
		
		
		%About amount of training data
		%About PLDA dimension
		%About in-domain data
		%About augmentation
		%About recording wise DER, comment on domains that did better
		%About the predicted number of speakers
		
	\section{Further Work}