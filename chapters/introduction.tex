\chapter{Introduction}

\section{Speaker Diarization}
Speaker diarization is commonly known as the task of finding out ``who spoke when?" in an audio recording with an unknown amount of speakers. It aims to split the recording into segments according to their speaker identity. These segments can also be overlapping. It acts as an important upstream preprocessing step for several tasks in speech processing. For example, it can be used in automatic transcription services to find all spoken segments and the speaker identities for them. The segments can then be passed to an automatic speech recognition (ASR) system to recognize the words. Furthermore, this also allows speaker-adapted ASR models to be used in order to improve the accuracy of ASR.

With increase in computing power, speech processing technologies have achieved incredible advances in the past decade that were not possible earlier. This has increased interest in automatic transcription technologies that can be used to automatically index the enormous amount of audio and video information that is generated in the modern world. This creates the creation of search engines that search audio files for information, just text documents. Examples of such queries can be: "which speakers tend to dominate a conversation?",  "which speakers are most likely to interrupt others?" and "fetch all segments spoken by a particular speaker". Since speaker diarization is an important part in any transcription system, there is a great deal of research interest in the area.

Diarization is not an easy problem since the output is affected by several factors like the application domain (broadcast news, meetings, telephone audio, internet audio, restaurant speech, clinical recordings etc), types and quality of microphones used (boom, lapel, far-field), inter-channel synchronization problems, overlapping speech, etc. These days, most of the research focuses on the meeting speech domain, since most problems that exist in speech recognition are encountered in this domain. The meeting scenario is thus often termed as ``speech recognition complete". But focusing on only one domain can lead to problems. It makes it hard to compare diarization systems that are trained for different domains. In the worst case it also causes overfitting to the domain that they are trained on.

The DIHARD challenge was created to establish standard datasets for diarization that have a good amount of domain variability. Systems that are trained only for a single domain are expected to perform poorly on these datasets. This is where the word ``hard" comes from in the name. The DIHARD datasets span several domains of speech like broadcast, meeting, telephone, restaurant, courtroom, YouTube speech etc.

\section{Motivation and Objectives}
Creating a diarization system for the DIHARD challenge can be a rewarding experience since it gives a chance to learn about state-of-the-art speaker diarization techniques. Thus the main aim of the project is to build such a system using the Kaldi toolkit \cite{povey2011kaldi} that works within the rules of the 2019 DIHARD challenge. Systems from last year's challenge (2018) can be used as a reference. The focus is to explore different possible configurations to get the best performing system.

\section{Report Outline}
In this chapter we briefly described the concepts in speaker diarization. Chapter 2 covers more detail on the various steps of a speaker diarization system and the Kaldi toolkit. Chapter 3 describes the structure and rules of the DIHARD challenge and the datasets involved. Chapter 4 explains how the baseline system supplied with the 2019 DIHARD challenge works. Chapter 5 describes different experiments that were performed throughout the project along with results. Chapter 6 concludes the report.