\chapter{Stages of Speaker Diarization}
This chapter discusses the general steps of a speaker diarization system and some of the recent techniques that are commonly used these days. In all diarization systems, the basic approach is to first extract the segments containing speech from the audio, further divide the segments into subsegments consisting of only one speaker, represent each subsegment using an embedding, and cluster the subsegments together so that each cluster represents unique speaker.

\section{Speech Enhancement}
Any speech system needs to address the problem of environmental noise in audio recordings. It is important to remove as much noise as possible, but also speaker information loss should be kept to a minimum. If done right, this stage results in increased performance in subsequent stages. If not, artefacts in denoised speech reduce diarization performance. In recent years, deep learning methods have partially solved the problem of artefacts, but generalization ability in varied domains is still a problem. This stage is not mandatory, but can be helpful in certain conditions.

\section{Feature Extraction}
The first important step in any speech system is to represent the speech recording using a sequence of feature vectors. This representation is much more compact as it only uses a few thousand parameters for every second of audio, compared to 44,100 samples in a raw waveform sampled at 44.1 KHz. The most commonly used features are Mel Frequency Cepstral Coefficients (MFCCs). Speech production can be thought of as filtering of the sound produced by the vocal cords by the shape of the human vocal tract. The shape is influenced by the positions of the tongue, teeth, lips, velum etc and determines what sound comes out. Since the shape of the vocal tract is manifested in the envelope of the short time power spectrum of the speech signal, the job of the MFCCs can be thought of as to represent this envelope accurately.

As a first step, overlapping frames are extracted from the digitized signal using a sliding window. FFT is applied on each frame to get the short time power spectrum of the frame.

\section{Speech Activity Detection}
Speech activity detection (SAD) extracts the segments in the audio that contain speech, and discards the rest. This is important because speaker diarization is only concerned with assigning speaker identities to speech segments, and does not do anything with non-speech segments. It is also possible to consider all the non-speech segments to be coming from a hypothetical new speaker which has its own cluster after clustering, but that does not result in good performance as the amount of variability possible in non-speech sounds is too high. Therefore, doing a separate SAD step before diarization is the standard method.

There are two goals of a good SAD system - keep missed speech to a minimum, and keep false alarm speech to a minimum. The first causes under-clustering and might cause speech segments being ignored or detect too few speakers, while the second pollutes clusters and degrades the diarization output. If the diarization system is used as a frontend for ASR, these errors cause word deletion and word insertion errors respectively.

Since this is a binary classification problem, it can be solved by using simple thresholding on the frame energy level. The MFCC features have the log energy of the frame as the first coefficient which can be used for this. The Kaldi program \ttvar{compute-vad} uses energy thresholding to do SAD.

Statistical model-based approaches are much more popular and are trained with lots of diverse external data. Typically Long Short Term Memory networks (LSTMs) or Time Delay Neural Networks (TDNNs) are used for this task.

\section{Segmentation}

The goal of the segmentation task is to further divide the speech segments found after the SAD step into smaller segments such that there are no speaker turns within any of these subsegments. Speaker turns are defined to be the points in the audio where the set of talking speakers changes. For example within a speech segment, the point where spkr1 changes into spkr2 (spkr1 finished talking and spkr2 started immediately) would be a speaker turn because the set of talking speakers changes from <spkr1> to <spkr2>. Similarly, if spkr2 starts talking without waiting for spkr1 to end (causing an overlap) the set changes from <spkr1> to <spkr1,spkr2>, which is also a speaker turn.

There are basically two ways to do segmentation. The first way is to try and automatically detect speaker turns and divide the segments at these points. The classical aproach for doing this uses a sliding window, and compares consecutive windows.	The comparision decides whether the two windows are better accounted by two separate models (different speaker sets) or single model (same speaker set) using an emperically determined threshold. Many distance metrics exist for this decision, for example the $\delta$ Bayesian Information Criterion (BIC) metric.

The second way is to divide the segments uniformly into very small subsegments (1-2 seconds) so that it is unlikely that the set of speakers changes within that segment.

It is not clear which of these ways yields better results. Uniform segmentation approaches are reported to work better for x-vectors \cite{patino2018odessa}.

\section{Clustering}
	As the most important step of the diarization process, clustering works on the whole audio recording and groups together segments that belong to the same speaker. In the ideal case, all the segments belong to a speaker exist in the same cluster, and the number of clusters is equal to the actual number of speakers. The clustering process needs a distance-like similarity measure to exist between pairs of segments. Each segment can be represented by a point in vector space or a statistical model. This representation acts as the speaker representation for the segment, where the similarity between any two representations is lower if the segments have speech from the same set of speakers.
	
	Since the speaker verification and speaker recognition fields also use speaker models to capture speaker information, these models are adopted for clustering for diarization. The best performing models are outlined below.
	
	\subsection{Speaker Representation}
		\subsubsection{Gaussian Mixture Models}
		Gaussian Mixture Models (GMMs) are generative models that can be used for modeling multivariate data. In GMMs, the probability of a data point is given by the weighted combination of the probabilities from multivariate Gaussian distributions having their own mean and covariance matrices.
		The goal is to train a GMM to represent each segment. For this, the feature vectors belonging to a segment can simply be pooled together and the GMM parameters can be learned using the Expectation-Maximization (EM) algorithm. But this is a problem because the number of feature vectors available from the segment would likely be insufficient to obtain a good estimate of the GMM parameters. This is because the number of GMMs is usually in the order of 512, 1024 or even 2048. To overcome this problem, a Universal Background Model (UBM) is trained using a large amount speech from the general population. This UBM is later adapted to each target segment using a Maximum Apriori (MAP) adaptation, resulting in an adapted GMM for each segment.
		Now that we have a GMM to represent each segment, we can use different statistical similarity measures that can act as a distance metric that can be used for clustering. The Kullback-Leibler (KL) divergence is a measure that estimates the distance between two random distributions. The cross likelihood ratio is given by the following.
		
		$$ CLR(S1, S2) = \log\frac{P(S1|M1)}{P(S1|M2)} + \log\frac{P(S2|M1)}{P(S2|M2)} $$
		
		Where $S_1$ and $S_2$ are the segments that are being compared, and $M_1$ and $M_2$ are their corresponding GMMs. It can be seen that if the segments come from the same speaker, the denominators increase, and the distance decreases.
		
		Later experiments found that only the means in the adapted GMMs carry most of the useful speaker information, the mixture weights and covariance matrices have too much variability to be of any use. Hence the means of GMMs were concatenated into a single vector called a GMM supervector and simpler distance measures like cosine distance and Mahalanobis distance were used as distance metrics for clustering.

		\subsubsection{i-vectors}
		I-vectors were introduced as a reduced dimension representation of the GMM supervector using factor analysis.
		
		$$ m_s = m_u + Tw_s $$
		
		$m_s$ and $m_u$ are the adapted supervector for a segment $S$ and the UBM supervector, respectively. $w_s$ is the i-vector of the segment $s$. $T$ is the ``total variability matrix" which projects the supervector down to the i-vector representation. $T$ is estimated from the training data using the EM algorithm.
		
		\subsubsection{x-vectors}
		X-vectors were introduced in \cite{snyder2018x}, where a DNN is trained to discriminate between speakers and map variable-length utterances to fixed dimensional embeddings called x-vectors. Unlike the i-vector training, where the projection matrix $T$ is learned in an unsupervised way, DNNs require speaker labels to train.
				
	\subsection{Agglomerative Hierarchical Clustering}
		Agglomerative Hierarchical Clustering (HAC) is the most commonly used clustering algorithm for diarization. It utilizes a bottom up approach in which the clustering is initialized with many more clusters than the actual number of speakers. It aims at reducing the number of clusters by 1 in each iteration by merging two of the most similar clusters. Many initializations are possible, some use k-means and many use uniform initialization.

	\subsection{Distance metrics}
		\subsubsection{PLDA}
		Probabilistic Linear Discriminant Analysis (PLDA) is the state-of-the-art scoring technique for diarization. It is used to generate a similarity scores between each pair of segments in a recording which are stored in an affinity matrix. This matrix is fed to the clustering as input.
		
		PLDA is a probabilistic extension of Linear Discriminant Analysis (LDA). LDA is similiar to Principal Component Analysis (PCA) where eigenvalues and eigenvectors of the covariance matrix are used to project data onto a lower dimension that maximizes the variance within the data. Unlike PCA, whose goal is data representation, the goal of LDA is class separability. Thus LDA is class-aware unlike PCA. Just like LDA, PLDA projects the data onto a lower dimension while minimizing discriminitative ability by maximizing the ratio of between-class and within-class variance.
		
		%http://www.odyssey2016.org/papers/pdfs_stamped/12.pdf
		%https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7078610&tag=1 ---------------> diagram idea also.

\section{Evaluation}

	\subsubsection{Diarization Error Rate}
		This metric is most popular for evaluating diarization. It was introduced for the NIST Rich Transcription Spring 2003 Evaluation (RT-03S). Basically, it is the total percentage of reference speaker time that is not correctly attributed to a speaker. ``Correctly attributed" relates to the optimal mapping between reference and system speakers. The DER formula is given below.
		
		$$ DER = \frac{FA + MISS + ERROR}{TOTAL} $$
		
		$TOTAL$ is the sum of durations of all reference speaker segments.
		$FA$ is the system speaker time that is not attributed to a reference speaker. This is when the system misinterprests a non-speech segment as speech.
		$MISS$ is the reference speaker time that is not accounted for by a system speaker. This is when the system misses a speech segment and classifies it as non-speech.
		$ERROR$ is the total system speaker time that is attributed to the wrong speaker. This is when the system incorrectly guesses the identity of a speaker from a voiced segment.
		
		It is easy to see that the best possible DER is 0\%. Also if FA is always zero, 100\% is the upper limit for DER. Because of its definition, speakers with longer segments tend to contribute more to DER than speakers with shorter segments.
		
		The NIST evaluations have a practice in which a forgiveness collar is applied to either sides of reference segments, to get rid of errors from inconsistent human annotations and uncertainty about when a speaker begins or ends. These collars are not scored. This does not apply for DIHARD and there are no forgiveness collars used. Overlapping speech is also evaluated - so the ideal system is expected to output overlapping segments if two speakers overlap in the source audio. A system that generates only flat segmentations (no overlap) will have higher DER because the overlapping segments could give rise to missed speech and speaker errors.

\section{Kaldi toolkit}
	\subsection{Introduction}
	The Kaldi speech recognition toolkit was started in 2009 at JHU with an aim to create a modern, well-engineered general purpose speech toolkit with a permissive license. Other aims of the project were to have a finite-state transducer (FST) based framework and have extensive linear algebra support. The toolkit depends on some external libraries that are freely avaialble - OpenFST, BLAS and LAPACK. The toolkit includes programs written in C++ that wrap these libraries, which are in turn called from bash/python scripts that can be collectively used to create complete recipes that do a specific job like speech/speaker recognition, diarization etc.
	
	Kaldi includes ready-to-use complete recipes for popular and widely available datasets such as those provided by the Linguistic Data Consortium (LDC). They are available as subdirectories of the \ttvar{egs} directory in Kaldi's root directory.
	
	\begin{verbatim}
	(base) [acq18mh@snarl ~/workspace]$ ls kaldi/egs
	README.txt            casia_hwdb               fisher_swbd
	aidatatang_200zh      chime1                   formosa
	aishell               chime2                   gale_arabic
	aishell2              chime3                   gale_mandarin
	ami                   chime4                   gp
	an4                   chime5                   heroico
	apiai_decode          cifar                    hkust
	aspire                commonvoice              hub4_english
	aurora4               csj                      hub4_spanish
	babel                 dihard2-voxceleb         iam
	babel_multilang       dihard_2018              iban
	bentham               fame                     ifnenit
	bn_music_speech       farsdat                  librispeech
	callhome_diarization  fisher_callhome_spanish  lre
	callhome_egyptian     fisher_english           lre07
	\end{verbatim}
	
	The C++ executables are have specific functionality so can be chained together in a typical Unix fashion to create complex pipelines. Given below is the simplified view of the Kaldi architecture.
	
	\subsection{Briew Overview of a Kaldi Recipe}
	This section describes some important parts of a typical Kaldi recipe. JHU's recipe for the DIHARD 2018 challenge \ttvar{egs/dihard_2018} is used as reference. It looks like the following.
	
	\begin{verbatim}
(base) [acq18mh@snarl kaldi]$ ls -l egs/dihard_2018
-rw-r--r--. acq18mh mini Jun 24 21:43 README.txt
-rwxr-xr-x. acq18mh mini Aug 26 03:26 cmd.sh
drwxr-sr-x. acq18mh mini Aug 19 01:04 conf
drwxr-sr-x. acq18mh mini Sep  7 19:21 data
lrwxrwxrwx. acq18mh mini Jun 24 21:43 diarization ->
 ../../callhome_diarization/v1/diarization
drwxr-sr-x. acq18mh mini Sep  7 19:20 exp
drwxr-sr-x. acq18mh mini Aug 30 01:16 local
drwxr-sr-x. acq18mh mini Sep  7 19:20 mfcc
-rwxr-xr-x. acq18mh mini Jun 24 21:43 path.sh
-rwxr-xr-x. acq18mh mini Aug 26 03:26 run.sh
lrwxrwxrwx. acq18mh mini Jun 24 21:43 sid -> ../../sre08/v1/sid
lrwxrwxrwx. acq18mh mini Jun 24 21:43 steps -> ../../wsj/s5/steps
lrwxrwxrwx. acq18mh mini Jun 24 21:43 utils -> ../../wsj/s5/utils
	\end{verbatim}
	
	Most Kaldi recipes have a toplevel script called ``run.sh" in the recipe directory. This is the starting point of the recipe. There are several scripts that are reused across many recipes. In many cases this is achieved by creating symlinks to other recipes, as seen by the arrows in the above listing. This recipe reuses scripts from the \ttvar{callhome_diarization}, \ttvar{wsj} and \ttvar{sre08} recipes. Scripts that are specific to this recipe are placed in \ttvar{local}, configuration files are placed in \ttvar{conf} and \ttvar{exp} contains files needed or created during the experiment. The \ttvar{data} directory is important because it holds a lot of information about the data that the experiment works on.
	
	\begin{verbatim}
	(base) [acq18mh@snarl dihard_2018]$ ls -l data
	-rw-r--r--.  1 acq18mh mini 3909183 Aug 28 03:32 feats.scp
	-rw-r--r--.  1 acq18mh mini    1642 Aug 28 03:31 reco2num_spk
	-rw-r--r--.  1 acq18mh mini 1768875 Aug 28 03:31 rttm
	-rw-r--r--.  1 acq18mh mini  808834 Aug 28 03:32 segments
	-rw-r--r--.  1 acq18mh mini  289353 Aug 28 03:32 spk2utt
	drwxr-sr-x. 42 acq18mh mini    4096 Aug 28 03:32 split40
	-rw-r--r--.  1 acq18mh mini  423522 Aug 28 03:32 utt2dur
	-rw-r--r--.  1 acq18mh mini  392200 Aug 28 03:32 utt2num_frames
	-rw-r--r--.  1 acq18mh mini  465297 Aug 28 03:32 utt2spk
	-rw-r--r--.  1 acq18mh mini   27388 Aug 28 03:32 wav.scp
	-rw-r--r--.  1 acq18mh mini   27388 Aug 28 03:32 vad.scp
	\end{verbatim}
	
	There are 3 files that must be created manually here: wav.scp, utt2spk, segments. The wav.scp file holds information about the actual audio files and how to extract data from them in a usable form. The utt2spk file contains identifiers for all utterances mapped to the identity of the speaker who speaks them. The spk2utt file is the opposite and is created automatically. The segments file holds information about how to extract the utterances from the audio files. The feats.scp file is created after feature extraction and tells where to find the features. The vad.scp file is created after voice activity detection (or SAD) and contains a binary decision about each frame being speech or non-speech.
	